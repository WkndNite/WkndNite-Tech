---
title: 计算层软件架构技术如何发展而来？
description: 计算层软件架构
sidebar:
    order: 2
---

## 计算层的软件架构技术挑战

---

### 单机计算模式的变革

-   `单机任务` 是最初采用的计算模式
-   `升级处理器 / 并行计算 ` 提升计算能力
-   `服务器` 解决掉想脱离客户机载体的计算需求
-   `集群 / 消息队列 / 负载调度` 应对用户规模激增问题
-   `多机房 / 冗余 / 异地多活 / 降级` 应对用户规模爆炸问题

也就是说，从计算复杂性的角度来讲——我们越来越需要**高性能、高扩展、高可用的计算模式**。

### 分布式计算模式的兴起

鉴于大型应用中业务需求的爆炸式增长、技术的不断演进导致系统异构化严重以及业务与技术的沟通存在鸿沟—— **传统垂直架构改造的核心，就是要对应用进行服务化。** 而服务化改造使用的核心技术，就是分布式服务框架。

然而，在应用从集中转向分布式的过程中，依旧存在以下问题：

-   系统开发维护成本高，部署效率低，应用数量膨胀，数据库连接数持续变高。
-   代码复用难度大，导致开发、测试、维护等工作烦、难、杂。
-   难以应对敏捷持续交付的挑战。

因此，大规模系统架构的设计一般原则就是**尽可能地拆分**，以达到更好的独立扩展与伸缩、更灵活的部署、更好的隔离和容错、更高的开发效率。

-   **业务纵向拆分**：按业务进行梳理，根据业务的特性把应用拆开，不同的业务模块独立部署。
-   **业务横向拆分**：将核心的、公共的业务拆分出来，通过分布式服务框架对业务进行服务化，消费者通过标准的契约来消费这些服务。服务提供者独立打包、部署和演进，与消费者解藕。

然而，在业务拆分之后服务数增多，**服务治理问题**成为了亟待解决的问题。服务治理的目标是**有效管控服务，提升服务运行质量，防止业务服务代码架构腐化。**服务治理要解决的主要问题如下：

-   **生命周期的管理**：服务上线随意，线上服务鱼龙混杂，上线容易下线难。需要规范化上线审批、测试发布流程、下线通知等。
-   **服务容量的规划**：需要能够采集服务调用性能、时延、成功率、系统资源占用等综合性能指标，分析并识别服务容量瓶颈，合理分配服务容量（计算、数据、网络等资源）。
-   **运行时的治理**：流量陡增时的非核心业务 SLA 降级；缓存失效，数据 I/O 开销陡增导致业务失败，需要在线调大服务调用超时时间；非核心服务故障时，业务应放行并执行本地降级逻辑。
-   **服务安全的保障**：服务调用鉴权、服务调用加密、服务调用防重放、服务调用限流、服务调用熔断、服务调用降级等。

### 非功能度量指标

-   **性能**：系统的性能指标包括 CPU 速度（MIPS）、RT 响应时间、吞吐量（MIPS / TFLOPS / TPS / QPS）、网络带宽（MbPS）、并发数、网络延时 等。
-   **可用性**：系统的可用性指标包括 MTBF、MTTR、MTTF、SLA、SLI、SLO、SLR 等。
-   **扩展性**：系统的扩展性指标包括水平扩展、垂直扩展、弹性扩展、伸缩性、容量规划、负载均衡、分布式存储、分布式计算等。

:::tip[相关概念]
TPS（Transactions Per Second）是指每秒钟系统处理的事务数，而 QPS（Queries Per Second）是指每秒钟系统处理的查询数。在数据库中，TPS 通常指的是事务数，而 QPS 通常指的是查询数。

MTTF（Mean Time To Failure）是指平均故障时间，MTTR（Mean Time To Repair）是指平均修复时间，MTBF（Mean Time Between Failures）是指平均故障间隔时间。

其中关系可以表示成：MTBF = MTTF + MTTR。系统可用性 = MTBF / (MTBF + MTTR)。在实际应用中，有些概念会将 MTBF 计算过程中再加一个 MTTD（Mean Time To Detect）。

这么看还是一团乱麻的，可以参考 [这篇文章](https://blog.csdn.net/yunhua_lee/article/details/121674703)。
:::

## 分布式编程模型

---

“摩尔定律”，即计算机硬件性能每隔 18 个月就会翻一番，而价格却会降低一半。这个定律在 2004 年左右开始失效，因为 CPU 的频率已经达到物理极限，无法再通过提高频率来提升性能。

因此，计算机硬件性能的提升开始转向多核处理器和并行计算。

谷歌公司的 **MapReduce** 和 **Hadoop** 等分布式计算框架应运而生，它们通过将计算任务分发到多个处理器上并行执行，从而实现了性能的大幅提升。

> 在 MapReduce 出现之前，已经有了 MPI 这样的并行计算框架。然而，MPI 的编程模型相对复杂，需要程序员手动管理并行计算中的数据分发和通信，这给程序员带来了很大的负担。


下面表格展示了 MapReduce 和 MPI 的主要区别：
| 特性         | MapReduce                   | MPI                           |
| ------------ | --------------------------- | ----------------------------- |
| 编程模型     | Map 和 Reduce 函数          | 点对点通信和集合操作          |
| 容错性       | 自动容错，任务失败自动重试  | 需要手动处理错误和容错        |
| 扩展性       | 自动扩展，支持大规模集群    | 需要手动管理节点和任务分配    |
| 编程难度     | 简单，适合大数据处理        | 复杂，需要手动管理并行计算    |
| 适用场景     | 批处理、非实时、数据密集型  | 实时、细粒度计算、计算密集型  |
| 硬件/价格/扩展性 | 刀片服务器、高速网、SAN，价格贵，扩展性差 | 普通PC机，便宜，扩展性好 |
