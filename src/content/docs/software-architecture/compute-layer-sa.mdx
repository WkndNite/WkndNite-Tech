---
title: 计算层软件架构技术如何发展而来？
description: 计算层软件架构
sidebar:
    order: 2
---

计算层的软件架构技术将从挑战与历史、分布式编程模型、负载均衡、消息队列和分布式服务框架等方面进行介绍。

## 计算层的软件架构技术挑战

---

### 单机计算模式的变革

-   `单机任务` 是最初采用的计算模式
-   `升级处理器 / 并行计算 ` 提升计算能力
-   `服务器` 解决掉想脱离客户机载体的计算需求
-   `集群 / 消息队列 / 负载调度` 应对用户规模激增问题
-   `多机房 / 冗余 / 异地多活 / 降级` 应对用户规模爆炸问题

也就是说，从计算复杂性的角度来讲——我们越来越需要**高性能、高扩展、高可用的计算模式**。

### 分布式计算模式的兴起

鉴于大型应用中业务需求的爆炸式增长、技术的不断演进导致系统异构化严重以及业务与技术的沟通存在鸿沟—— **传统垂直架构改造的核心，就是要对应用进行服务化。** 而服务化改造使用的核心技术，就是分布式服务框架。

然而，在应用从集中转向分布式的过程中，依旧存在以下问题：

-   系统开发维护成本高，部署效率低，应用数量膨胀，数据库连接数持续变高。
-   代码复用难度大，导致开发、测试、维护等工作烦、难、杂。
-   难以应对敏捷持续交付的挑战。

因此，大规模系统架构的设计一般原则就是**尽可能地拆分**，以达到更好的独立扩展与伸缩、更灵活的部署、更好的隔离和容错、更高的开发效率。

-   **业务纵向拆分**：按业务进行梳理，根据业务的特性把应用拆开，不同的业务模块独立部署。
-   **业务横向拆分**：将核心的、公共的业务拆分出来，通过分布式服务框架对业务进行服务化，消费者通过标准的契约来消费这些服务。服务提供者独立打包、部署和演进，与消费者解藕。

然而，在业务拆分之后服务数增多，**服务治理问题**成为了亟待解决的问题。服务治理的目标是**有效管控服务，提升服务运行质量，防止业务服务代码架构腐化。**服务治理要解决的主要问题如下：

-   **生命周期的管理**：服务上线随意，线上服务鱼龙混杂，上线容易下线难。需要规范化上线审批、测试发布流程、下线通知等。
-   **服务容量的规划**：需要能够采集服务调用性能、时延、成功率、系统资源占用等综合性能指标，分析并识别服务容量瓶颈，合理分配服务容量（计算、数据、网络等资源）。
-   **运行时的治理**：流量陡增时的非核心业务 SLA 降级；缓存失效，数据 I/O 开销陡增导致业务失败，需要在线调大服务调用超时时间；非核心服务故障时，业务应放行并执行本地降级逻辑。
-   **服务安全的保障**：服务调用鉴权、服务调用加密、服务调用防重放、服务调用限流、服务调用熔断、服务调用降级等。

### 非功能度量指标

-   **性能**：系统的性能指标包括 CPU 速度（MIPS）、RT 响应时间、吞吐量（MIPS / TFLOPS / TPS / QPS）、网络带宽（MbPS）、并发数、网络延时 等。
-   **可用性**：系统的可用性指标包括 MTBF、MTTR、MTTF、SLA、SLI、SLO、SLR 等。
-   **扩展性**：系统的扩展性指标包括水平扩展、垂直扩展、弹性扩展、伸缩性、容量规划、负载均衡、分布式存储、分布式计算等。

:::tip[相关概念]
TPS（Transactions Per Second）是指每秒钟系统处理的事务数，而 QPS（Queries Per Second）是指每秒钟系统处理的查询数。在数据库中，TPS 通常指的是事务数，而 QPS 通常指的是查询数。

MTTF（Mean Time To Failure）是指平均故障时间，MTTR（Mean Time To Repair）是指平均修复时间，MTBF（Mean Time Between Failures）是指平均故障间隔时间。

其中关系可以表示成：MTBF = MTTF + MTTR。系统可用性 = MTBF / (MTBF + MTTR)。在实际应用中，有些概念会将 MTBF 计算过程中再加一个 MTTD（Mean Time To Detect）。

这么看还是一团乱麻的，可以参考 [这篇文章](https://blog.csdn.net/yunhua_lee/article/details/121674703)。
:::

## 分布式编程模型

---

### 分布式编程模式

“摩尔定律”，即计算机硬件性能每隔 18 个月就会翻一番，而价格却会降低一半。这个定律在 2004 年左右开始失效，因为 CPU 的频率已经达到物理极限，无法再通过提高频率来提升性能。

因此，计算机硬件性能的提升开始转向多核处理器和并行计算。

谷歌公司的 **MapReduce** 和 **Hadoop** 等分布式计算框架应运而生，它们通过将计算任务分发到多个处理器上并行执行，从而实现了性能的大幅提升。

> 在 MapReduce 出现之前，已经有了 MPI 这样的并行计算框架。然而，MPI 的编程模型相对复杂，需要程序员手动管理并行计算中的数据分发和通信，这给程序员带来了很大的负担。

下面表格展示了 MapReduce 和 MPI 的主要区别：
| 特性 | MapReduce | MPI |
| ------------ | --------------------------- | ----------------------------- |
| 编程模型 | Map 和 Reduce 函数 | 点对点通信和集合操作 |
| 容错性 | 自动容错，任务失败自动重试 | 需要手动处理错误和容错 |
| 扩展性 | 自动扩展，支持大规模集群 | 需要手动管理节点和任务分配 |
| 编程难度 | 简单，适合大数据处理 | 复杂，需要手动管理并行计算 |
| 适用场景 | 批处理、非实时、数据密集型 | 实时、细粒度计算、计算密集型 |
| 硬件/价格/扩展性 | 刀片服务器、高速网、SAN，价格贵，扩展性差 | 普通 PC 机，便宜，扩展性好 |

### 分布式计算集群

分布式文件系统把文件分布存储到多个计算机节点上，成千上万的计算机节点构成计算机集群——与之前使用多个处理器和专用高级硬件的并行化处理装置不同的是，目前的分布式文件系统所采用的计算机集群，都是由普通硬件构成的，这就大大降低了硬件上的开销。

### 分布式文件系统的结构

分布式文件系统在物理结构上是由计算机集群中的多个节点构成的，这些节点分为两类——一类叫**主节点**，另一类叫**从节点**或者**数据节点**。

### MapReduce 模型

#### 简介

MapReduce 将复杂的、运行于大规模集群上的并行计算过程高度抽象成了两个函数——Map 和 Reduce。

-   MapReduce 模型的核心思想是**分而治之**，即将一个大任务分解成多个小的任务，然后将这些小的任务分发给多个计算节点并行处理，最后将各个节点的处理结果汇总起来得到最终结果。
-   MapReduce 设计的一个理念就是**将计算向数据迁移**，而不是将数据向计算迁移。也就是说，MapReduce 模型会将数据分发到各个计算节点上，而不是将计算任务分发到各个计算节点上。这样做的好处是可以减少网络通信的开销，提高计算效率。
-   MapReduce 框架采用了 **Master/Slave** 架构，由一个 Master 节点和多个 Slave 节点组成。Master 节点运行 JobTracker，负责调度和管理各个 Slave 节点上的 TaskTracker。Slave 节点运行 TaskTracker，负责执行具体的计算任务。

:::MapReduce 和 Hadoop
MapReduce 是一种编程模型，而 Hadoop 是一个实现了 MapReduce 编程模型的分布式计算框架。Hadoop 使用 MapReduce 模型来处理大规模数据集，并将计算任务分发到多个计算节点上并行处理。Hadoop 是用 Java 编写的，但是也可以使用其他语言编写 MapReduce 程序。
:::

#### 组成部分

MapReduce 体系结构主要由四个部分组成：**Client**、**JobTracker**、**TaskTracker** 和 **Task**。

-   **Client**：用户编写的 MapReduce 程序通过 Client 提交到 JobTracker 端，用户可通过 Client 提供的一些接口查看作业运行状态。
-   **JobTracker**：JobTracker 负责资源监控和作业调度，JobTracker 监控所有 TaskTracker 与 Job 的健康状况，一旦发现失败，就将相应的任务转移到其他节点。JobTracker 会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器（TaskScheduler），而调度器会在资源出现空闲时，选择合适的任务去使用这些资源。
-   **TaskTracker**：TaskTracker 会周期性地通过“心跳”将本节点上资源的使用情况和任务的运行进度汇报给 JobTracker，同时接收 JobTracker 发送过来的命令并执行相应的操作（如启动新任务、杀死任务等）。TaskTracker 使用“slot”等量划分本节点上的资源量（CPU、内存等）。一个 Task 获取到一个 slot 后才有机会运行，而 Hadoop 调度器的作用就是将各个 TaskTracker 上的空闲 slot 分配给 Task 使用。slot 分为 Map slot 和 Reduce slot 两种，分别供 MapTask 和 ReduceTask 使用。
-   **Task**：Task 分为 Map Task 和 Reduce Task 两种，均由 TaskTracker 启动。Map Task 将输入数据分割成多个小块，每个小块由一个 Map 任务处理。Map 任务将输入数据转换为键值对，并将这些键值对发送给 Reduce 任务。Map 任务通常使用哈希函数将键值对分配给不同的 Reduce 任务。Reduce 任务接收来自 Shuffle 阶段的键值对，并对这些键值对进行合并和计算。

#### 工作流程

MapReduce 模型的工作流程可以分为以下几个步骤：

1. **输入数据**：\
   输入数据被分割成多个数据块，每个数据块的大小通常为 64MB 到 128MB。这些数据块被存储在分布式文件系统中，如 HDFS。
2. **Split 阶段**：\
   Split 阶段将输入数据分割成多个小块，每个小块由一个 Map 任务处理。\
   HDFS 以固定大小的 block 为基本单位存储数据，而 MapReduce 处理单位是 split。\
   split 是一个逻辑概念，它只包含一些元数据信息，比如数据起始位置、数据长度、数据所在节点等。它的划分方法完全由用户自己决定。
3. **Map 阶段**：\
   Map 阶段将输入数据分割成多个小块，每个小块由一个 Map 任务处理。Map 任务将输入数据转换为键值对，并将这些键值对发送给 Reduce 任务。Map 任务通常使用哈希函数将键值对分配给不同的 Reduce 任务。\
   每个 Map 任务会被分配一个缓存，默认会设置 1000，并设置溢写比例为 0.8，当缓存达到 80%时，就会触发溢写操作，将数据写入磁盘。Map 阶段进行数据的排序、合并与归并，最终将数据写入磁盘。

    > Hadoop 为每个 split 创建一个 Map 任务，split 的多少决定了 Map 任务的数目。大多数情况下，理想的分片大小是一个 HDFS 块（默认 64MB）。

4. **Shuffle 阶段**：\
   Shuffle 阶段将 Map 任务输出的键值对按照键进行排序，并将相同键的键值对发送给同一个 Reduce 任务。Shuffle 阶段通常使用哈希函数将键值对分配给不同的 Reduce 任务。
5. **Reduce 阶段**：\
   Reduce 任务接收来自 Shuffle 阶段的键值对，并对这些键值对进行合并和计算。Reduce 任务通常使用哈希函数将键值对分配给不同的 Reduce 任务。\
   Reduce 通过 RPC 向 JobTracker 询问 MapTask 的输出结果，JobTracker 会告诉 ReduceTask 哪些机器存有相应的数据，然后 ReduceTask 通过 HTTP 方式从这些机器上获取数据。\
   获取数据后，ReduceTask 会对数据归并、合并、写入磁盘。数据很少会直接从缓存归并，并输出给 Reduce。

    > 最优的 Reduce 任务个数取决于集群中可用的 reduce 任务槽(slot)的数目——通常设置比 reduce 任务槽数目稍微小一些的 Reduce 任务个数（这样可以预留一些系统资源处理可能发生的错误）。

6. **输出数据**：\
   Reduce 任务将计算结果输出到分布式文件系统中，这些结果可以被其他应用程序读取和使用。

:::tip[概述]

-   不同的 Map 任务之间不会通信，它们各自独立地处理输入数据。
-   不同的 Reduce 任务之间也不会通信，它们各自独立地处理来自 Map 任务的输出数据。
-   用户不能显式地从一台机器向另一台机器发送消息，所有的数据交换都是通过 MapReduce 框架进行的。
    :::

#### 问题

-   JobTracker 是 MapReduce 的集中处理点，存在单点故障。
-   JobTracker 完成了太多的任务，造成了过多的资源消耗。
-   当 MapReduce job 非常多时，会造成很大的内存开销：业界总结 Hadoop 的 MapReduce 只能支持 4000 节点主机的上限。
-   在 TaskTracker 端，以 map/reduce task 的数目作为资源的表示过于简单，没有考虑到 cpu/内存的占用情况，如果两个大内存消耗的任务被调度到了一块，很容易出现溢出。

#### MapReduce on Yarn

YARN（Yet Another Resource Negotiator）是 Hadoop 2.x 引入的一个新的资源管理框架，它将资源管理和作业调度解耦，使得 MapReduce 可以与其他计算框架（如 Spark、Tez 等）共享集群资源。

-   设计优点

        -   这个设计大大减小了 JobTracker（也就是现在的 ResourceManager）的资源消耗，并且让监测每一个 Job 子任务 (tasks) 状态的程序分布式化了，更安全、更优美。
        -   另外，在新版中，ApplicationMaster 是一个可变更的部分，用户可以对不同的编程模型写自己的 ApplicationMaster，让更多类型的编程模型能够跑在 Hadoop 集群中。
        -   能够支持不同的编程模型。
        -   对于资源的表示以内存为单位（在目前版本的 Yarn 中，没有考虑 CPU 的占用），比之前以剩余 slot 数目更合理。
        -   既然资源表示成内存量，那就没有了之前的 mapslot/reduceslot 分开造成集群资源闲置的尴尬情况了。

## 负载均衡

---

### 负载均衡

负载均衡（Load Balancing）是一种网络架构，它通过将网络流量分配到多个服务器来提高系统的可用性和性能。**负载均衡器**（Load Balancer）是用于实现负载均衡的设备或软件。负载均衡器分为**DNS 负载均衡**、**硬件负载均衡**和**软件负载均衡**三种类型。

负载均衡器通常位于网络中的关键位置，如防火墙、路由器或交换机等。它通过监听来自客户端的请求，并根据一定的算法将请求分配给后端的服务器。负载均衡器可以确保后端服务器能够均匀地处理请求，从而提高系统的整体性能和可用性。

#### DNS 负载均衡

-   优点：实现简单，成本低；就近访问，提高访问速度。
-   缺点：
    -   DNS 解析结果可能存在缓存，导致负载不均衡；
    -   扩展性差，无法根据业务需求动态调整负载；
    -   分配策略比较简单，不能区分服务器差异，不能感知服务器状态。

#### 硬件负载均衡

目前业界典型的硬件负载均衡设备有两款：F5 和 A10。

-   优点：

        -   功能强大，支持全面的负载均衡算法，支持全局负载均衡；
        -   性能强大，支持百万级并发；
        -   稳定性高；
        -   支持安全防护。

-   缺点：
    -   成本高，价格昂贵；
    -   扩展性差，无法满足业务快速变化的需求；
    -   维护成本高，需要专业人员进行维护。

#### 软件负载均衡

软件负载均衡器通常运行在服务器上，通过软件实现负载均衡功能。常见的软件负载均衡器包括 Nginx、HAProxy、LVS 等。Nginx 是 7 层负载均衡器，HAProxy 是 4 层负载均衡器，LVS 是 4 层负载均衡器。

:::tip
层数的区别在于**协议**和**灵活性**。Nginx 支持 HTTP、E-mail 协议，而 LVS 和协议无关，几乎所有应用都可以支持。
:::

-   优点：简单、便宜、灵活；
-   缺点：与硬件负载均衡相比，性能较差，功能不够强大，一般不具备安全防护功能。

#### 负载均衡典型架构

-   **地理级别负载均衡**：当用户访问的时候，根据用户地理位置，将用户请求分发到最近的服务器上，从而提高访问速度和用户体验。
-   **集群级别负载均衡**：F5 收到请求后，进行集群级别的负载均衡。
-   **机器级别负载均衡**：Nginx 收到用户请求后，将用户请求发送给集群里面的某台服务器。

#### 负载均衡的算法

分配标准有以下几种：**任务数平分**、**负载均衡**、**性能最优**、**Hash**。

-   **任务数平分**：将任务均匀地分配给所有可用的服务器。
-   **负载均衡**：根据服务器的负载情况，将任务分配给负载较低的服务器。
-   **性能最优**：根据服务器的性能指标，将任务分配给性能最优的服务器。
-   **Hash**：根据请求的某个属性（如 IP 地址、URL 等）进行哈希运算，将任务分配给哈希值对应的服务器。

> 具体有哪些算法呢？

-   轮询（Round Robin）：将请求按顺序分配给服务器。
-   加权轮询（Weighted Round Robin）：根据服务器的权重，将请求按比例分配给服务器。
-   负载最低优先（Least Connections）：将请求分配给当前连接数最少的服务器。
-   性能最优（Best Performance）：将请求分配给响应时间最短的服务器。
-   IP Hash（IP Hash）：根据请求的 IP 地址进行哈希运算，将请求分配给哈希值对应的服务器。
-   URL Hash（URL Hash）：根据请求的 URL 进行哈希运算，将请求分配给哈希值对应的服务器。
-   ID Hash（ID Hash）：根据请求的 ID 进行哈希运算，将请求分配给哈希值对应的服务器。

### 消息队列

#### 通信模型

-   基于寻址类型，可以分为直接通信和间接通信。
-   基于阻塞类型，可以分为同步通信和异步通信。
-   基于缓存类型，可以分为瞬态通信和持久化通信。
-   基于内容类型，可以分为事件通信、指令通信、数据通信和流通信。
-   基于确认类型，可以分为无确认通信、有确认通信和可靠通信。
-   基于接收节点数，可以分为点对点通信、多播通信、任播通信、广播通信和地域性群播通信。
-   基于通讯方向，可以分为单向通信和半双工通信和全双工通信。
-   基于发起方，可以分为 Pull 通信和 Push 通信。
-   基于消息存储，可以分为瞬态通信和持久化通信。
